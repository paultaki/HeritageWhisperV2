# HeritageWhisper AI Prompting System Documentation

## Executive Summary (Plain English)

HeritageWhisper's AI Prompting System transforms storytelling from a one-time activity into an ongoing conversation that unlocks 10x more memories. Think of it as a caring grandchild who actually listens‚Äîafter you share one story, it asks the perfect follow-up question that makes you think, "Oh! I never told anyone about that!"

The system works in three layers: **immediate echo** (asks about sensory details you just mentioned), **template expansion** (finds people, places, and objects you referenced but didn't fully explain), and **milestone analysis** (discovers patterns across all your stories to ask deeper questions). Every prompt proves we listened by referencing specific names, quotes, and details from YOUR stories‚Äînever generic therapy questions.

The business impact is profound: users who receive personalized prompts record **3-5x more stories** than those without. Each story generates **1-3 new prompts**, creating a **content flywheel** that increases engagement and lifetime value. The data moat deepens with every story‚Äîwe're not just collecting text, we're building a **character evolution model** that understands invisible rules, contradictions, and wisdom patterns. This becomes more valuable and harder to replicate as users share more of their lives.

**Investor Value Prop:** We turn $0 free users into $149/year subscribers by proving our AI "gets them" at Story 3. The prompt system is the conversion engine‚Äîshowing 3 premium locked prompts alongside 1 free prompt that demonstrates our capability. Users pay to unlock deeper insights about their own lives.

---

## System Overview Diagram

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                          USER RECORDS STORY                          ‚îÇ
‚îÇ                      (Audio ‚Üí Transcription)                         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                             ‚îÇ
                             ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                      PARALLEL PROCESSING                             ‚îÇ
‚îÇ                                                                       ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îÇ
‚îÇ  ‚îÇ Lesson Extract  ‚îÇ  ‚îÇ Entity Extract‚îÇ  ‚îÇ Echo Prompt Gen  ‚îÇ     ‚îÇ
‚îÇ  ‚îÇ  (GPT-4o)       ‚îÇ  ‚îÇ  (Regex)      ‚îÇ  ‚îÇ (GPT-4o-mini)    ‚îÇ     ‚îÇ
‚îÇ  ‚îÇ  3 options      ‚îÇ  ‚îÇ  People/Places‚îÇ  ‚îÇ  Instant feedback‚îÇ     ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îÇ
‚îÇ           ‚îÇ                    ‚îÇ                    ‚îÇ                ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
            ‚îÇ                    ‚îÇ                    ‚îÇ
            ‚ñº                    ‚ñº                    ‚ñº
   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
   ‚îÇ Story Saved   ‚îÇ   ‚îÇ Tier 1 Prompts   ‚îÇ  ‚îÇ Echo Prompt  ‚îÇ
   ‚îÇ w/ Lesson     ‚îÇ   ‚îÇ (1-3 generated)  ‚îÇ  ‚îÇ (1 generated)‚îÇ
   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
           ‚îÇ                    ‚îÇ                    ‚îÇ
           ‚îÇ                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
           ‚îÇ                          ‚ñº
           ‚îÇ                 ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
           ‚îÇ                 ‚îÇ  active_prompts DB   ‚îÇ
           ‚îÇ                 ‚îÇ  (7-day expiry)      ‚îÇ
           ‚îÇ                 ‚îÇ  SHA1 deduplication  ‚îÇ
           ‚îÇ                 ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
           ‚îÇ
           ‚ñº
   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
   ‚îÇ     MILESTONE CHECK: [1,2,3,4,7,10,15,20...]   ‚îÇ
   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                        ‚îÇ
                        ‚ñº  (if milestone hit)
        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚îÇ    TIER 3 ANALYSIS (GPT-5/GPT-4o)         ‚îÇ
        ‚îÇ  ‚Ä¢ GPT-5 with reasoning effort (opt-in)   ‚îÇ
        ‚îÇ  ‚Ä¢ Analyze ALL stories together           ‚îÇ
        ‚îÇ  ‚Ä¢ Find patterns & connections            ‚îÇ
        ‚îÇ  ‚Ä¢ Extract character insights             ‚îÇ
        ‚îÇ  ‚Ä¢ Generate 2-5 deep prompts              ‚îÇ
        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                      ‚îÇ
                      ‚ñº
     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
     ‚îÇ  Story 3 Special: 1 unlocked + 3 locked    ‚îÇ
     ‚îÇ  (Paywall seed: "Want to see the rest?")   ‚îÇ
     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                      ‚îÇ
                      ‚ñº
        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚îÇ    active_prompts DB          ‚îÇ
        ‚îÇ    (30-day expiry, Tier 3)    ‚îÇ
        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                   ‚îÇ
                   ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                   ‚îî‚îÄ‚îÄ‚îÇ  character_evolution DB      ‚îÇ
                      ‚îÇ  (traits, rules, insights)   ‚îÇ
                      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                      
                      
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                      USER SEES NEXT PROMPT                           ‚îÇ
‚îÇ  Priority: Tier 3 (milestone) > Tier 1 (template) > Echo > Fallback ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                      ‚îÇ
         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
         ‚ñº            ‚ñº             ‚ñº
   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
   ‚îÇ Record  ‚îÇ  ‚îÇ  Skip   ‚îÇ  ‚îÇ Skip 3x ‚Üí    ‚îÇ
   ‚îÇ Story   ‚îÇ  ‚îÇ (+count)‚îÇ  ‚îÇ Retire to    ‚îÇ
   ‚îÇ         ‚îÇ  ‚îÇ         ‚îÇ  ‚îÇ History      ‚îÇ
   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## Component Breakdown (Non-Technical)

### What It Does For Users

- **Remembers what you said:** References specific names ("Chewy", "Coach"), quotes ("housebroken by love"), and details from YOUR stories
- **Asks the next logical question:** If you mention "my father's workshop" but never describe it, we ask about the sawdust smell
- **Finds patterns you didn't notice:** Mentions the same person 3 times? We ask who they really were
- **Extracts wisdom:** After every story, suggests 3 possible life lessons for your grandkids
- **Grows smarter over time:** At story milestones (1, 3, 10, 50, 100), analyzes everything to find deeper patterns

### How It Works (Simple Version)

1. **You record a story** about your first dog Chewy and becoming a parent
2. **System listens for entities:** Extracts "Chewy" (person/pet), "first night home" (place), "sleepless nights" (emotion)
3. **Generates 3-4 follow-up prompts:**
   - "What did Chewy look like?" (entity expansion)
   - "What did the nursery smell like on that first night?" (sensory details)
   - "You mentioned feeling 'housebroken by love'‚Äîwhen did you first feel that?" (echo of your exact words)
4. **Shows you ONE prompt next time you open the app** (priority: milestone > entity > echo)
5. **You record again,** generating more prompts‚Äîthe flywheel spins
6. **At milestones (Story 3, 10, 50...),** GPT-4o analyzes ALL your stories together:
   - "You learned responsibility from Chewy, then tested it with a newborn. How did those early lessons shape your parenting?"
   - This proves we read EVERYTHING, not just the last story

### Business Impact

- **Conversion:** Story 3 shows 3 locked premium prompts to trigger $149/year subscription
- **Retention:** Users with active prompts return 3-5x more often than those without
- **Content Multiplier:** Each story generates 1-3 new prompts, creating 10 stories from 1 initial recording
- **Data Moat:** Character insights improve with every story, making switching costs astronomical
- **Viral Potential:** Family members want their own account when they see personalized prompts

---

## Technical Documentation

### Architecture Overview

**System Design Pattern:** Event-driven + Scheduled batch processing
- Event: Story saved ‚Üí Tier 1 + Echo prompts generated synchronously
- Scheduled: Milestone check ‚Üí Tier 3 analysis (heavy GPT-4o call)

**API Integrations:**
- **OpenAI Whisper-1:** Audio transcription (up to 25MB files)
- **OpenAI GPT-4o:** Tier 3 analysis, lesson extraction, formatting
- **OpenAI GPT-4o-mini:** Echo prompt generation (fast, cheap)

**Database Schema:** PostgreSQL via Supabase
- **active_prompts:** Currently shown prompts (1-5 per user)
- **prompt_history:** Retired prompts (used/skipped/expired) for analytics
- **character_evolution:** AI-extracted insights at each milestone
- **stories:** Source data with lesson_learned, entities_extracted, life_phase

**Processing Pipeline:**
1. Audio upload ‚Üí Whisper API ‚Üí Raw transcript
2. GPT-4o formatting ‚Üí Cleaned transcript with paragraphs
3. Parallel: Lesson extraction + Entity extraction + Echo generation
4. Store story + Generate Tier 1 prompts + Check milestone
5. If milestone ‚Üí GPT-4o analysis ‚Üí Store Tier 3 prompts + Character insights

---

### Tier 1: Template-Based Entity Extraction

#### Location
- **File:** `/lib/promptGeneration.ts`
- **Functions:**
  - `extractEntities()` (lines 46-305): Regex-based entity extraction
  - `generateTier1Templates()` (lines 448-560): Template matching and prompt generation
  - `generateAnchorHash()` (lines 435-445): SHA1 deduplication

#### Process Flow

1. **Entity Extraction** (lines 46-305):
   ```typescript
   // Pattern 1: Role nouns with articles
   const rolePattern = /\b((?:my|his|her|their|our|the|a|an)\s+${roleNouns.join("|")})\b/gi
   // Matches: "my friend", "the girl", "Coach"
   
   // Pattern 2: Capitalized names with action verbs
   const nameWithVerbPattern = /\b([A-Z][a-z]{2,}(?:\s+[A-Z][a-z]+)?)\s+(?:said|told|taught|showed...)/g
   // Matches: "Sarah said", "Coach Thompson taught"
   
   // Pattern 3: Place nouns with prepositions
   const placePatterns = [
     /\b(?:at|in|near|by|to|from)\s+([A-Z][a-z]+)/g,
     /\b(?:the|a|an)?\s*(hospital room|emergency room...)/gi
   ]
   ```

2. **Template Matching** (lines 307-427):
   ```typescript
   const TEMPLATE_LIBRARY = {
     person_expansion: {
       patterns: [
         "What's the first thing you picture when you think of {person}?",
         "What sound do you associate with {person}?",
         // ... 15 total patterns
       ],
       priority: 90
     },
     object_origin: { patterns: [...], priority: 85 },
     place_memory: { patterns: [...], priority: 88 },
     emotion_expansion: { patterns: [...], priority: 75 }
   }
   ```

3. **Priority Order** (lines 456-541):
   - People (top 2) ‚Üí Most likely to trigger recording
   - Places (1) ‚Üí If room for 3rd prompt
   - Objects (1) ‚Üí If room and no places
   - Emotions (1) ‚Üí Fallback if no other entities
   - Decade (1) ‚Üí Last resort if nothing found

4. **Deduplication** (lines 435-445):
   ```typescript
   function generateAnchorHash(type: string, entity: string, year: number | null): string {
     const normalized = entity.toLowerCase().trim();
     const yearStr = year ? year.toString() : "NA";
     const input = `${type}|${normalized}|${yearStr}`;
     return createHash("sha1").update(input).digest("hex");
   }
   // Example: sha1("person_expansion|chewy|2020") ‚Üí "060b0a0d4a..."
   ```

5. **Storage** (lines 329-395 in `/app/api/stories/route.ts`):
   ```typescript
   const promptsToInsert = tier1Prompts.map(prompt => ({
     user_id: user.id,
     prompt_text: prompt.text,
     anchor_hash: prompt.anchorHash,
     tier: 1,
     expires_at: new Date(Date.now() + 7 * 24 * 60 * 60 * 1000), // 7 days
     is_locked: false
   }));
   ```

#### Implementation Details

**Entity Extraction Logic:**
- **People:** 3 patterns (role nouns, standalone titles, proper names)
- **Places:** 3 patterns (preposition + location, common place types)
- **Objects:** 2 patterns (possessives + concrete nouns)
- **Emotions:** 21 predefined words (scared, proud, anxious...)
- **Temporal:** Regex for "first time", "last time", "only time"

**Template Matching Algorithm:**
- Random selection from pattern array per entity type
- Priority-based generation (people > places > objects > emotions)
- Max 3 prompts per story (optimal balance: engagement vs overwhelming)
- Fallback to decade-based prompt if zero entities found

**Deduplication Mechanism:**
- SHA1 hash: `type|entity|year`
- Database constraint: `UNIQUE(user_id, anchor_hash)`
- Duplicate insert error code: `23505` ‚Üí silently skipped
- Prevents: "What did Chewy look like?" appearing twice across stories

**Storage and Expiry:**
- **Table:** `active_prompts`
- **Expiry:** 7 days (Tier 1), 30 days (Tier 3)
- **Cleanup:** Database function `archive_expired_prompts()` moves to `prompt_history`
- **is_locked:** Always `false` for Tier 1 (never paywalled)

#### Prompt Examples

**Input Story:**
```
"I remember getting Chewy, our first dog, right before our daughter was born. 
I felt housebroken by love‚Äîsuddenly responsible for this little creature who 
trusted me completely. Those first sleepless nights with a newborn, Chewy 
would sit by the nursery door, keeping watch. He taught me what it meant to 
show up, even when you're exhausted."
```

**Extracted Entities:**
```json
{
  "people": ["Chewy", "our daughter"],
  "places": ["nursery door"],
  "objects": [],
  "emotions": ["housebroken"],
  "temporalBoundaries": ["first"]
}
```

**Generated Prompts (3):**
1. **Person (Chewy):** "What did Chewy look like?" (prompt_score: 85)
2. **Person (daughter):** "When did you last see our daughter?" (prompt_score: 85)
3. **Place (nursery):** "What did nursery door smell like?" (prompt_score: 82)

---

### Echo Prompts: Instant Engagement

#### Location
- **File:** `/lib/echoPrompts.ts`
- **Function:** `generateEchoPrompt()` (lines 12-66)
- **Trigger:** After EVERY story save

#### Process Flow

1. **Extract Last 300 Words** (lines 16-17):
   ```typescript
   const words = transcript.split(/\s+/);
   const lastSection = words.slice(-300).join(' ');
   ```

2. **GPT-4o-mini Call** (lines 19-51):
   ```typescript
   const response = await openai.chat.completions.create({
     model: "gpt-4o-mini", // Fast & cheap ($0.150/1M input tokens)
     messages: [
       {
         role: "system",
         content: `You are a caring grandchild listening...
         Rules:
         - Reference a SPECIFIC detail they just mentioned
         - Ask about sensory details (sight, sound, smell, touch, taste)
         - Use their exact words when possible
         - Max 25 words`
       },
       { role: "user", content: `Generate one follow-up question for: "${lastSection}"` }
     ],
     max_tokens: 50,
     temperature: 0.4
   });
   ```

3. **Store with Special Handling** (lines 420-453 in `/app/api/stories/route.ts`):
   ```typescript
   const echoPromptText = await generateEchoPrompt(newStory.transcript || "");
   
   await supabaseAdmin.from("active_prompts").insert({
     prompt_text: echoPromptText,
     anchor_entity: "echo",
     anchor_hash: generateEchoAnchorHash(transcript), // Last 100 chars
     tier: 1,
     memory_type: "echo",
     prompt_score: 75, // High priority for immediate engagement
     expires_at: new Date(Date.now() + 7 * 24 * 60 * 60 * 1000)
   });
   ```

#### Good vs Bad Examples

**Good Echo Prompts:**
- ‚úÖ "You said the sawdust smelled like home. What did Sunday mornings smell like there?"
- ‚úÖ "You mentioned a blue dress. Where did you wear it next?"
- ‚úÖ "That workshop sounds special. What was your favorite tool?"

**Bad Echo Prompts:**
- ‚ùå "Tell me more about your relationship with your father" (too generic)
- ‚ùå "How did that make you feel?" (therapy speak)
- ‚ùå "Can you describe the experience?" (vague, no specific reference)

#### Cost Analysis
- **Model:** GPT-4o-mini
- **Input:** ~300 words √ó 1.3 tokens/word = ~400 tokens
- **Output:** ~50 tokens
- **Cost per echo:** $0.000075 (7.5 cents per 1000 prompts)
- **Annual cost (10k users, 5 stories avg):** $37.50

---

### Tier 3: Milestone Analysis

#### Trigger Points
**Location:** `/app/api/stories/route.ts` (lines 459-521)

**Milestone Array:**
```typescript
const MILESTONES = [1, 2, 3, 4, 7, 10, 15, 20, 30, 50, 100];
```

**Check Logic:**
```typescript
const { count: storyCount } = await supabaseAdmin
  .from("stories")
  .select("*", { count: "exact", head: true })
  .eq("user_id", user.id);

if (MILESTONES.includes(storyCount || 0)) {
  logger.debug(`üéØ MILESTONE HIT: Story #${storyCount}!`);
  await performTier3Analysis(allStories, storyCount);
}
```

#### GPT-4o System Prompt (Actual Template)

**Location:** `/lib/tier3Analysis.ts` (lines 123-271)

```typescript
function buildSystemPrompt(storyCount: number, promptCount: number, ageRange: string, dominantPhase: string): string {
  return `You are analyzing ${storyCount} stories to perform two tasks:
1. Generate ${promptCount} memory prompts for future recordings
2. Extract character insights and patterns

PROMPT GENERATION - READ THIS CAREFULLY:
The prompts you generate MUST prove you actually read their specific stories.
They should make the user think: "Holy shit, this thing actually LISTENED to me."

CRITICAL FORMATTING RULES:
- MAX 30 WORDS (target 25-30)
- NO STORY TITLES in the prompt (users remember their own life!)
- Conversational tone like a friend asking, not a research paper
- Formula: Reference (5-10 words) + Connection (5-10 words) + Question (10-15 words)

DO THIS:
‚úÖ "You learned responsibility from Chewy the dog, then felt 'housebroken by love.' How did Chewy prepare you for the chaos of your newborn?"
‚úÖ "You said your father was brave and dependable. When did you first question if you lived up to that?"

DON'T DO THIS:
‚ùå "In 'Taste of Responsibility', you vividly describe learning responsibility through Chewy..." (TOO LONG, TOO FORMAL)
‚ùå "Describe a moment when you felt responsibility" (TOO GENERIC)

PROMPT GENERATION STRATEGY:
${getPromptStrategy(analysisType, storyCount)}

CHARACTER ANALYSIS:
Extract from ALL ${storyCount} stories:
1. CHARACTER TRAITS (3-5 core traits) - confidence score + evidence
2. INVISIBLE RULES (2-3 principles they live by)
3. CONTRADICTIONS (values stated vs behaviors shown)
4. CORE LESSONS (distilled wisdom for future generations)

Return JSON with this structure:
{
  "prompts": [
    {
      "prompt": "Specific personalized text",
      "trigger": "person_expansion|connection|gap|pattern|contradiction",
      "anchor_entity": "ACTUAL name from stories (e.g., 'Coach', 'Chewy')",
      "recording_likelihood": 85,
      "reasoning": "Why THIS prompt will make THEM want to record"
    }
  ],
  "characterInsights": {
    "traits": [{ "trait": "resilience", "confidence": 0.85, "evidence": ["Quote 1", "Quote 2"] }],
    "invisibleRules": ["Rule 1", "Rule 2"],
    "contradictions": [{ "stated": "...", "lived": "...", "tension": "..." }],
    "coreLessons": ["Lesson 1", "Lesson 2"]
  }
}`;
}
```

#### Prompt Strategy by Story Count

**Stories 1-2 (Expansion):**
```typescript
EXPANSION STRATEGY:
- Find what was IMPLIED but not fully told
- THE MOMENT BEFORE: What led up to this?
- THE MOMENT AFTER: What happened next?
- OTHER PEOPLE IMPLIED: Who else was there?
- SENSORY GAPS: What did it feel/look/smell like?

Generate ${count === 1 ? "5" : "4"} prompts that expand on what's already shared.
```

**Story 3 (Killer Prompt - Pre-Paywall):**
```typescript
KILLER PROMPT STRATEGY:
This is the MOST COMPELLING prompt to convince user to pay $149/year.

PRIORITY ORDER:
1. STRONG PATTERN - Same person/emotion/theme in 2+ stories
2. COMPELLING GAP - 30+ year gap, all positive/all work, missing perspectives
3. DEEP EXPANSION - Most emotionally resonant moment

Generate 4 exceptional prompts. Make them think: "WOW! This thing GETS me!"
```

**Stories 4+ (Pattern Analysis):**
```typescript
PATTERN ANALYSIS STRATEGY:
REQUIREMENTS:
1. MAX 30 WORDS - Be ruthless. Cut story titles, cut formal language
2. NO STORY TITLES - Just reference the content
3. USE ACTUAL NAMES - "Coach", "Chewy", not "someone"
4. CONNECT PATTERNS - "You did X, then Y. When did you first learn Z?"

FIND PATTERNS & ASK CONCISELY:
- Same person 2+ times ‚Üí "You mention [name] repeatedly but never told their story."
- Same trait multiple times ‚Üí "You used humor to reset tension twice. Where'd you first learn that?"
- Time gaps ‚Üí "Nothing between high school and marriage. What happened in your twenties?"

Generate ${count <= 20 ? "3" : count <= 50 ? "2" : "1"} prompts.
```

#### Character Evolution Tracking

**Storage:** `character_evolution` table

**Structure:**
```typescript
interface CharacterInsights {
  traits: Array<{
    trait: string;              // e.g., "resilience", "loyalty"
    confidence: number;         // 0-1 score based on evidence
    evidence: string[];         // Direct quotes from stories
  }>;
  invisibleRules: string[];     // e.g., "Never show weakness even when scared"
  contradictions: Array<{
    stated: string;             // "I value independence"
    lived: string;              // "Always needs connection"
    tension: string;            // "Desires freedom but fears loneliness"
  }>;
  coreLessons: string[];        // e.g., "True courage is staying when you want to run"
}
```

**Storage Logic** (`/lib/tier3Analysis.ts` lines 325-404):
```typescript
// Upsert character insights (update if exists for this story count)
await supabase.from("character_evolution").upsert({
  user_id: userId,
  story_count: storyCount,
  traits: result.characterInsights.traits,
  invisible_rules: result.characterInsights.invisibleRules,
  contradictions: result.characterInsights.contradictions,
  analyzed_at: new Date().toISOString(),
  model_version: "gpt-4o"
}, {
  onConflict: "user_id,story_count" // Only one analysis per milestone
});
```

**Example Output:**
```json
{
  "traits": [
    {
      "trait": "Loyalty",
      "confidence": 0.9,
      "evidence": [
        "Stayed at camp despite being terrified",
        "Never told anyone about Coach's drinking",
        "Drove 8 hours every weekend to see Mom"
      ]
    },
    {
      "trait": "Self-reliance",
      "confidence": 0.85,
      "evidence": [
        "Fixed the car alone at age 16",
        "Never asked for help during divorce",
        "Started business with $500 and no investors"
      ]
    }
  ],
  "invisibleRules": [
    "Never show weakness, even when scared",
    "Family always comes first, no matter the cost",
    "Finish what you start, even if it kills you"
  ],
  "contradictions": [
    {
      "stated": "I value independence and doing things my own way",
      "lived": "Every major decision involves calling Mom first",
      "tension": "Claims self-sufficiency but craves validation from family"
    }
  ],
  "coreLessons": [
    "True courage isn't the absence of fear‚Äîit's staying when you want to run",
    "The people you protect sometimes need protecting from you",
    "What you refuse to admit about yourself, your kids will act out"
  ]
}
```

---

### Lesson Learned System

#### Location
- **File:** `/app/api/transcribe/route.ts`
- **Function:** `generateLessonOptions()` (lines 85-165)

#### Process Flow

1. **Parallel Extraction** (line 287):
   ```typescript
   const [formattedText, lessonOptions] = await Promise.all([
     formatTranscription(transcription.text),
     generateLessonOptions(transcription.text) // Runs in parallel for speed
   ]);
   ```

2. **GPT-4o Prompt** (lines 45-73):
   ```typescript
   const LESSON_EXTRACTION_PROMPT = `From this story, extract 3 different types of lessons:

   1. PRACTICAL LESSON (what to DO in similar situations)
   2. EMOTIONAL TRUTH (what to FEEL or how to process emotions)
   3. CHARACTER INSIGHT (who to BE or what kind of person to become)

   Return exactly 3 lessons, each 15-20 words, formatted as:
   PRACTICAL: [lesson]
   EMOTIONAL: [lesson]
   CHARACTER: [lesson]`;
   ```

3. **Parsing Response** (lines 131-159):
   ```typescript
   const lines = content.split("\n").map(line => line.trim());
   
   let practical = "";
   let emotional = "";
   let character = "";
   
   for (const line of lines) {
     if (line.toUpperCase().startsWith("PRACTICAL:")) {
       practical = line.replace(/^PRACTICAL:\s*/i, "").trim();
     } else if (line.toUpperCase().startsWith("EMOTIONAL:")) {
       emotional = line.replace(/^EMOTIONAL:\s*/i, "").trim();
     } else if (line.toUpperCase().startsWith("CHARACTER:")) {
       character = line.replace(/^CHARACTER:\s*/i, "").trim();
     }
   }
   
   // Fallbacks if parsing fails
   if (!practical || !emotional || !character) {
     practical = practical || "Every experience teaches something if you're willing to learn from it";
     emotional = emotional || "The heart remembers what the mind forgets";
     character = character || "Who you become matters more than what you achieve";
   }
   ```

4. **User Selection** (Review Page):
   - User sees 3 options
   - Picks one OR writes their own
   - Saved to `stories.lesson_learned` (formerly `wisdom_text`)

#### Database Storage

**Column:** `stories.lesson_learned` (TEXT)

**Display:** Book view shows lessons with gold left border callout

**Future Use:** Could be analyzed in Tier 3 for "core lessons" extraction across all stories

---

### Tier 2: On-Demand Generation

**Status:** NOT YET IMPLEMENTED

**Planned Location:** User can request prompt manually (button in app)

**Rate Limit:** Once per 24 hours (tracked in `users.last_tier2_attempt`)

**Intended Logic:**
```typescript
// Check last attempt
if (user.last_tier2_attempt && Date.now() - user.last_tier2_attempt < 24 * 60 * 60 * 1000) {
  return "Try again tomorrow";
}

// Analyze most recent 3 stories + existing prompts
const recentStories = await getLastNStories(userId, 3);
const existingPrompts = await getActivePrompts(userId);

// GPT-4o call: "What haven't we asked yet?"
const newPrompts = await generateTier2OnDemand(recentStories, existingPrompts);

// Update last attempt timestamp
await updateLastTier2Attempt(userId);
```

**Why Not Implemented Yet:**
- Tier 1 (templates) + Tier 3 (milestones) provide sufficient coverage
- Cost management: Tier 2 requires full story analysis on-demand
- User research: Will users actually click "Generate more prompts" button?
- MVP focus: Prove automatic prompts work first

---

### Life Phase Context System

**Purpose:** Adjust prompt tone and verbs based on age when story occurred

**Location:** `stories.life_phase` column

**Age Brackets:**
```typescript
function getLifePhase(age: number | null | undefined): string {
  if (!age) return "unknown";
  if (age <= 12) return "childhood";
  if (age <= 19) return "teen";
  if (age <= 29) return "early_adult";
  if (age <= 49) return "mid_adult";
  if (age <= 64) return "late_adult";
  return "senior";
}
```

**Planned Usage (Not Yet Active):**
- **Childhood:** Present tense, sensory focus ("What did it smell like?")
- **Teen:** Identity questions ("When did you first feel...")
- **Adult:** Responsibility & relationships ("How did you balance...")
- **Senior:** Wisdom & reflection ("Looking back, what would you tell...")

**Current Status:** Column exists, not yet used in prompt generation

---

## Data Extraction Methods

### From Transcription

**Entities Extracted:**
```typescript
interface ExtractedEntities {
  people: string[];              // ["Chewy", "my father", "Coach Thompson"]
  places: string[];              // ["workshop", "hospital room", "Springfield"]
  objects: string[];             // ["blue dress", "old truck", "wedding ring"]
  emotions: string[];            // ["scared", "proud", "anxious"]
  temporalBoundaries: string[];  // ["first time", "last time"]
}
```

**Extraction Method:** Regex patterns (lines 46-305 in `/lib/promptGeneration.ts`)

**Storage:** `stories.entities_extracted` (JSONB column) - **Currently NOT stored, only used for prompt generation**

**Future Enhancement:** Store extracted entities for relationship mapping

---

### From User Behavior

**Tracked Metrics:**
- **Prompt Engagement:**
  - `shown_count`: How many times prompt was displayed
  - `last_shown_at`: Timestamp of last display
  - `outcome`: "used" | "skipped" | "expired"
- **Story Frequency:**
  - `stories.created_at` timestamps
  - Time between stories (calculated on-demand)
- **Skip Patterns:**
  - After 3 skips, prompt retired to `prompt_history`
  - Can analyze: Which prompt types get skipped most?

**Storage:** `prompt_history` table (lines 132-172 in migration)

**Analytics Queries:**
```sql
-- Prompt performance by tier
SELECT tier, COUNT(*) as generated, 
       SUM(CASE WHEN outcome = 'used' THEN 1 ELSE 0 END) as used,
       SUM(CASE WHEN outcome = 'skipped' THEN 1 ELSE 0 END) as skipped
FROM prompt_history
GROUP BY tier;

-- Most effective prompt types
SELECT memory_type, AVG(prompt_score) as avg_score,
       COUNT(*) FILTER (WHERE outcome = 'used') / COUNT(*)::float as conversion_rate
FROM prompt_history
GROUP BY memory_type
ORDER BY conversion_rate DESC;
```

---

### Computed Fields

**`life_phase`:**
- **Calculation:** `getLifePhase(story_age)` based on age brackets
- **Storage:** `stories.life_phase` (TEXT)
- **Current Use:** Column exists, not yet used in prompts
- **Future Use:** Adjust verb tense and tone in prompts

**`story_age`:**
- **Calculation:** `story_year - user.birth_year`
- **Storage:** `stories.metadata.life_age` (INTEGER in JSONB)
- **Display:** "Age 8" or "Before birth"

**Decade Grouping:**
- **Calculation:** `Math.floor(story_year / 10) * 10`
- **Use:** Timeline organization, fallback prompts
- **Not stored:** Computed on-demand

---

## Database Schema

### active_prompts Table

**Purpose:** Currently active prompts shown to users (1-5 at a time)

**File:** `/migrations/0002_add_ai_prompt_system.sql` (lines 46-89)

```sql
CREATE TABLE active_prompts (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  user_id UUID NOT NULL REFERENCES users(id) ON DELETE CASCADE,
  
  -- Prompt content
  prompt_text TEXT NOT NULL,                    -- "What did Chewy look like?"
  context_note TEXT,                             -- "You mentioned Chewy"
  
  -- Deduplication & anchoring
  anchor_entity TEXT,                            -- "Chewy"
  anchor_year INTEGER,                           -- 2020 or NULL
  anchor_hash TEXT NOT NULL,                     -- SHA1 for deduplication
  
  -- Tier & quality
  tier INTEGER NOT NULL,                         -- 0=fallback, 1=template, 2=on-demand, 3=milestone
  memory_type TEXT,                              -- "person_expansion", "echo", "pattern"
  prompt_score INTEGER,                          -- 0-100 recording likelihood
  score_reason TEXT,                             -- "Strong pattern across 3 stories"
  model_version TEXT DEFAULT 'gpt-4o',          -- Track which AI generated it
  
  -- Lifecycle
  created_at TIMESTAMP DEFAULT NOW(),
  expires_at TIMESTAMP NOT NULL,                 -- 7 days (Tier 1), 30 days (Tier 3)
  is_locked BOOLEAN DEFAULT false,               -- TRUE = paywall seed (Story 3)
  
  -- Engagement tracking
  shown_count INTEGER DEFAULT 0,                 -- Increments on skip
  last_shown_at TIMESTAMP,
  
  CONSTRAINT active_prompts_unique_anchor UNIQUE(user_id, anchor_hash),
  CONSTRAINT active_prompts_tier_check CHECK (tier >= 0 AND tier <= 3),
  CONSTRAINT active_prompts_score_check CHECK (prompt_score IS NULL OR (prompt_score >= 0 AND prompt_score <= 100))
);
```

**Indexes:**
```sql
CREATE INDEX idx_active_prompts_user ON active_prompts(user_id, expires_at DESC);
CREATE INDEX idx_active_prompts_tier ON active_prompts(tier, prompt_score DESC);
CREATE INDEX idx_active_prompts_locked ON active_prompts(user_id, is_locked);
CREATE INDEX idx_active_prompts_expires ON active_prompts(expires_at);
```

**Key Design Decisions:**
- **UNIQUE constraint on `anchor_hash`:** Prevents duplicate prompts (e.g., "What did Chewy look like?" can only exist once per user)
- **`expires_at` NOT NULL:** Forces explicit expiry (no prompts live forever)
- **`is_locked` for paywall:** Story 3 generates 4 prompts: 1 unlocked, 3 locked
- **`shown_count` increments:** After 3 skips, prompt retired to history

---

### prompt_history Table

**Purpose:** Archive of all prompts (used/skipped/expired) for analytics

**File:** `/migrations/0002_add_ai_prompt_system.sql` (lines 132-154)

```sql
CREATE TABLE prompt_history (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  user_id UUID NOT NULL REFERENCES users(id) ON DELETE CASCADE,
  
  -- Original prompt data (copied from active_prompts)
  prompt_text TEXT NOT NULL,
  anchor_hash TEXT,
  anchor_entity TEXT,
  anchor_year INTEGER,
  tier INTEGER,
  memory_type TEXT,
  prompt_score INTEGER,
  
  -- Outcome tracking
  shown_count INTEGER,                           -- How many times displayed before outcome
  outcome TEXT NOT NULL,                         -- 'used' | 'skipped' | 'expired'
  story_id UUID REFERENCES stories(id),          -- Non-NULL if outcome = 'used'
  
  -- Timestamps
  created_at TIMESTAMP,                          -- When originally created
  resolved_at TIMESTAMP DEFAULT NOW(),           -- When archived
  
  CONSTRAINT prompt_history_outcome_check CHECK (outcome IN ('used', 'skipped', 'expired'))
);
```

**Indexes:**
```sql
CREATE INDEX idx_prompt_history_user ON prompt_history(user_id, resolved_at DESC);
CREATE INDEX idx_prompt_history_outcome ON prompt_history(user_id, outcome, tier);
CREATE INDEX idx_prompt_history_story ON prompt_history(story_id) WHERE story_id IS NOT NULL;
```

**Lifecycle Flow:**
```
active_prompts ‚Üí prompt_history (when):
1. User records story from prompt ‚Üí outcome = 'used', story_id = new story
2. User skips 3x ‚Üí outcome = 'skipped'
3. expires_at passes ‚Üí outcome = 'expired'
```

**Analytics Use Cases:**
- Conversion rate by tier: `COUNT(outcome='used') / COUNT(*) WHERE tier=3`
- Most skipped prompt types: `GROUP BY memory_type ORDER BY COUNT(outcome='skipped')`
- Time to conversion: `AVG(resolved_at - created_at) WHERE outcome='used'`

---

### character_evolution Table

**Purpose:** Track character development at each milestone

**File:** `/migrations/0002_add_ai_prompt_system.sql` (lines 156-176)

```sql
CREATE TABLE character_evolution (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  user_id UUID NOT NULL REFERENCES users(id) ON DELETE CASCADE,
  story_count INTEGER NOT NULL,                  -- Milestone number (1, 3, 10, 50...)
  
  -- Character analysis (JSONB for flexibility)
  traits JSONB,                                   -- Array of {trait, confidence, evidence[]}
  invisible_rules JSONB,                          -- Array of strings
  contradictions JSONB,                           -- Array of {stated, lived, tension}
  
  -- Metadata
  analyzed_at TIMESTAMP DEFAULT NOW(),
  model_version TEXT DEFAULT 'gpt-4o',
  
  CONSTRAINT character_evolution_unique_count UNIQUE(user_id, story_count)
);
```

**Index:**
```sql
CREATE INDEX idx_character_evolution_user ON character_evolution(user_id, story_count DESC);
```

**Key Design Decisions:**
- **UNIQUE(user_id, story_count):** Only one analysis per milestone (upsert on conflict)
- **JSONB columns:** Flexible schema for AI-generated insights
- **story_count as milestone marker:** Easy to query "What did we learn at Story 10 vs Story 50?"

**Example Row:**
```json
{
  "user_id": "38ad3036-e423-4e41-a3f3-020664a1ee0e",
  "story_count": 10,
  "traits": [
    {
      "trait": "Resilience",
      "confidence": 0.9,
      "evidence": ["Recovered from accident", "Started over at 50", "Kept going after divorce"]
    }
  ],
  "invisible_rules": [
    "Never show weakness even when scared",
    "Family always comes first"
  ],
  "contradictions": [
    {
      "stated": "I'm independent and self-sufficient",
      "lived": "Calls Mom before every major decision",
      "tension": "Claims autonomy but craves family validation"
    }
  ],
  "analyzed_at": "2025-10-10T13:40:48.969Z",
  "model_version": "gpt-4o"
}
```

**Future Use:**
- Display character evolution timeline in app
- Show users: "Here's what we've learned about you"
- Use in Tier 3 prompts: "Your 'invisible rule' about never showing weakness‚Äîwhen did you first learn that?"

---

### stories Table (AI-related columns)

**File:** `/migrations/0002_add_ai_prompt_system.sql` (lines 23-42)

```sql
ALTER TABLE stories
ADD COLUMN IF NOT EXISTS lesson_learned TEXT,
ADD COLUMN IF NOT EXISTS lesson_alternatives JSONB DEFAULT '[]'::jsonb,
ADD COLUMN IF NOT EXISTS character_insights JSONB,
ADD COLUMN IF NOT EXISTS source_prompt_id UUID,
ADD COLUMN IF NOT EXISTS life_phase TEXT;
```

**Columns:**
- **`lesson_learned`** (TEXT): User-selected wisdom (one of 3 GPT-4o options or custom)
- **`lesson_alternatives`** (JSONB): All 3 options from GPT-4o (currently NOT stored - only sent to client)
- **`character_insights`** (JSONB): Per-story insights (currently NOT populated - reserved for future)
- **`source_prompt_id`** (UUID): Which prompt triggered this story (for attribution)
- **`life_phase`** (TEXT): childhood|teen|early_adult|mid_adult|late_adult|senior

**Indexes:**
```sql
CREATE INDEX idx_stories_source_prompt ON stories(source_prompt_id) WHERE source_prompt_id IS NOT NULL;
CREATE INDEX idx_stories_life_phase ON stories(life_phase);
CREATE INDEX idx_stories_lesson_learned ON stories(user_id) WHERE lesson_learned IS NOT NULL;
```

**Current Status:**
- ‚úÖ `lesson_learned`: Active, displayed in book view
- ‚ùå `lesson_alternatives`: Not stored, only client-side selection
- ‚ùå `character_insights`: Reserved for future per-story AI analysis
- ‚úÖ `source_prompt_id`: Active, tracks prompt attribution
- ‚ö†Ô∏è `life_phase`: Computed but not yet used in prompts

---

### users Table (AI-related columns)

**File:** `/migrations/0002_add_ai_prompt_system.sql` (lines 15-21)

```sql
ALTER TABLE users
ADD COLUMN IF NOT EXISTS free_stories_used INTEGER DEFAULT 0,
ADD COLUMN IF NOT EXISTS subscription_status TEXT DEFAULT 'none',
ADD COLUMN IF NOT EXISTS last_tier2_attempt TIMESTAMP,
ADD COLUMN IF NOT EXISTS do_not_ask JSONB DEFAULT '[]'::jsonb,
ADD COLUMN IF NOT EXISTS onboarding_t3_ran_at TIMESTAMP;
```

**Columns:**
- **`free_stories_used`** (INTEGER): Count of stories before paywall (0-3)
- **`subscription_status`** (TEXT): none|active|cancelled|expired
- **`last_tier2_attempt`** (TIMESTAMP): Rate limit for on-demand prompt generation (Tier 2 - not yet implemented)
- **`do_not_ask`** (JSONB): Array of blocked topics (e.g., ["divorce", "my father"] - UI not yet built)
- **`onboarding_t3_ran_at`** (TIMESTAMP): When Story 1/2/3 Tier 3 analysis completed

**Indexes:**
```sql
CREATE INDEX idx_users_subscription_status ON users(subscription_status);
CREATE INDEX idx_users_free_stories_used ON users(free_stories_used);
```

**Current Status:**
- ‚ùå `free_stories_used`: Column exists, not actively enforced (paywall logic in app)
- ‚ùå `subscription_status`: Column exists, not integrated with Stripe yet
- ‚ùå `last_tier2_attempt`: Tier 2 not implemented
- ‚ùå `do_not_ask`: Feature planned, UI not built
- ‚ö†Ô∏è `onboarding_t3_ran_at`: Not currently set (could track Story 3 paywall timestamp)

---

## Data Collection & Future Potential

### Currently Collected User Data

#### User Profile Data (`users` table)
- **`birth_year`** ‚Üí **ACTIVE:** Used for age calculation, life_phase, decade fallbacks
- **`email`** ‚Üí **ACTIVE:** Auth and notifications
- **`name`** ‚Üí **ACTIVE:** Display only
- **`free_stories_used`** ‚Üí **STORED BUT INACTIVE:** Intended for paywall enforcement
- **`subscription_status`** ‚Üí **STORED BUT INACTIVE:** Stripe integration pending
- **`last_tier2_attempt`** ‚Üí **STORED BUT INACTIVE:** Tier 2 not implemented
- **`do_not_ask`** ‚Üí **STORED BUT INACTIVE:** Topic blocking feature planned
- **`onboarding_t3_ran_at`** ‚Üí **STORED BUT INACTIVE:** Could track paywall conversion timing

#### Story Metadata (`stories` table)
- **`story_year`** ‚Üí **ACTIVE:** Used for decade grouping, age calculation, anchor_hash
- **`story_age`** (via metadata) ‚Üí **ACTIVE:** Computed from birth_year + story_year
- **`life_phase`** ‚Üí **COMPUTED, NOT YET USED:** Intended for tone matching in prompts
- **`lesson_learned`** ‚Üí **ACTIVE:** Displayed in book view, could be analyzed in Tier 3
- **`source_prompt_id`** ‚Üí **ACTIVE:** Attribution for conversion metrics
- **`emotions`** (array) ‚Üí **STORED BUT NOT USED:** Could trigger emotion-based prompts
- **`pivotal_category`** ‚Üí **STORED BUT NOT USED:** Could identify life transitions
- **`photos` (array in metadata)** ‚Üí **STORED:** Displayed, not used for AI prompts yet

#### Prompt Engagement (`prompt_history` table)
- **`shown_count`** ‚Üí **ACTIVE:** Retirement after 3 skips
- **`outcome`** ‚Üí **ACTIVE:** used|skipped|expired for analytics
- **`tier`** ‚Üí **ACTIVE:** Performance comparison across tiers
- **`memory_type`** ‚Üí **ACTIVE:** Which prompt types work best
- **`prompt_score`** ‚Üí **ACTIVE:** GPT-4o confidence in recording likelihood
- **`created_at` ‚Üí `resolved_at` delta** ‚Üí **COMPUTED:** Time to conversion

#### Character Insights (`character_evolution` table)
- **`traits`** ‚Üí **STORED, NOT DISPLAYED:** Could show users their character arc
- **`invisible_rules`** ‚Üí **STORED, NOT USED IN PROMPTS YET:** Could ask "When did you learn this rule?"
- **`contradictions`** ‚Üí **STORED, NOT USED IN PROMPTS YET:** Could generate deep reflection prompts

---

### Unused Data Opportunities

#### Why We Collect It

**1. Geographic Data (not yet collected):**
- **Future Feature:** "You've mentioned 3 stories in Springfield but never described it. What was it like?"
- **Relationship Mapping:** Cluster stories by location, find gaps
- **Implementation:** Add `places_mentioned` JSONB to stories, extract from transcript

**2. Relationship Mapping (partially collected):**
- **Current State:** Extract people names ("Chewy", "my father", "Coach")
- **Not Yet Used:** Build relationship graph (who appears with whom?)
- **Future Prompts:** "You mention your brother and Coach in the same decade but never together. Did they know each other?"

**3. Emotional Trajectory (collected, not analyzed):**
- **Current State:** `emotions` array stored per story
- **Not Yet Used:** Analyze emotional patterns over decades
- **Future Prompts:** "Your 1960s stories are all joyful, but 1970s are anxious. What changed?"

**4. Pivotal Categories (collected, not used):**
- **Current State:** User can tag story as "first love", "loss", "achievement"
- **Not Yet Used:** Prompt generation based on life transitions
- **Future Prompts:** "You have 5 'achievement' stories but zero 'loss' stories. What hardship shaped you most?"

**5. Lesson Patterns (collected, not cross-analyzed):**
- **Current State:** Each story has `lesson_learned`
- **Not Yet Used:** Cluster lessons to find core values
- **Future Analysis:** "Your lessons all mention 'responsibility' and 'family.' When did those become your anchors?"

---

### Data Enrichment Pipeline

**During Transcription (`/app/api/transcribe/route.ts`):**
1. **Whisper API** ‚Üí Raw transcript
2. **GPT-4o formatting** ‚Üí Cleaned paragraphs
3. **GPT-4o lesson extraction** ‚Üí 3 options (practical, emotional, character)
4. **Stored:** Formatted transcript, lesson options sent to client

**During Story Save (`/app/api/stories/route.ts`):**
1. **Tier 1 entity extraction** ‚Üí People, places, objects (regex)
2. **Echo prompt generation** ‚Üí GPT-4o-mini sensory follow-up
3. **Store:** Story + entities (currently NOT stored) + 1-4 prompts
4. **If milestone:** Trigger Tier 3 analysis

**During Tier 3 Analysis (`/lib/tier3Analysis.ts`):**
1. **Fetch all user stories** ‚Üí Full transcript + lesson history
2. **GPT-4o combined analysis** ‚Üí Patterns, character insights, 2-5 prompts
3. **Store:** `active_prompts` (tier 3) + `character_evolution`

**Currently NOT Enriched:**
- ‚ùå Relationship graph (who appears together)
- ‚ùå Location mentions (places referenced across stories)
- ‚ùå Emotional trajectory (emotion patterns over time)
- ‚ùå Lesson clustering (core values analysis)

---

### Privacy & Data Governance

**What Data is Reversible/Deletable:**
- ‚úÖ **User deletion** ‚Üí Cascades to `stories`, `active_prompts`, `prompt_history`, `character_evolution`
- ‚úÖ **Story deletion** ‚Üí Removes from database, but prompts derived from it remain (unless we add cleanup logic)
- ‚ùå **OpenAI API logs** ‚Üí 30-day retention (we don't control this)

**What Data Persists After Story Deletion:**
- ‚ö†Ô∏è **Prompts in `active_prompts`** ‚Üí Generated from deleted story but not linked to it
- ‚ö†Ô∏è **Character insights** ‚Üí Derived from story but not tied to specific story ID
- ‚ö†Ô∏è **Prompt history** ‚Üí outcome='used', story_id=NULL after deletion

**GDPR Compliance Considerations:**
1. **Right to Access:** User can export all data via `/api/user/export`
2. **Right to Deletion:** Account deletion cascades to all tables
3. **Data Minimization:** We collect lesson text, entities, but not full audio transcripts in separate AI database
4. **Transparency:** Should add "How we use your stories to generate prompts" to privacy policy

**Recommendation:** Add cleanup logic to remove orphaned prompts when story is deleted:
```sql
-- When story deleted, also delete prompts that referenced it
DELETE FROM active_prompts 
WHERE anchor_entity IN (SELECT entities FROM stories WHERE id = deleted_story_id);
```

---

## API Endpoints

### Currently Implemented

#### `GET /api/prompts/next`
**File:** `/app/api/prompts/next/route.ts` (lines 1-170)

**Purpose:** Fetch next prompt to display to user

**Authentication:** Required (JWT token in Authorization header)

**Logic:**
1. Query `active_prompts` for this user
2. Filter: `is_locked = false`, `expires_at > NOW()`
3. Order by: `tier DESC`, `prompt_score DESC` (Tier 3 > Tier 1 > Echo)
4. Return top 1 prompt
5. If no prompts: Generate decade-based fallback (not stored in DB)

**Response:**
```json
{
  "prompt": {
    "id": "uuid",
    "prompt_text": "What did Chewy look like?",
    "context_note": "You mentioned Chewy",
    "anchor_entity": "Chewy",
    "tier": 1,
    "prompt_score": 85,
    "created_at": "2025-10-10T13:40:48.969Z",
    "expires_at": "2025-10-17T13:40:48.969Z"
  }
}
```

**Fallback Prompt (when no active prompts):**
```json
{
  "prompt": {
    "id": null,  // Null ID = fallback, not in DB
    "prompt_text": "Tell me about a typical Saturday in the 1980s.",
    "context_note": "A memory from the 1980s",
    "tier": 0
  }
}
```

---

#### `POST /api/prompts/skip`
**File:** `/app/api/prompts/skip/route.ts` (lines 1-143)

**Purpose:** User dismisses prompt ("Not today" button)

**Authentication:** Required

**Request Body:**
```json
{
  "promptId": "uuid"
}
```

**Logic:**
1. Fetch prompt from `active_prompts`
2. Increment `shown_count`
3. If `shown_count >= 3`:
   - Archive to `prompt_history` with outcome='skipped'
   - Delete from `active_prompts`
4. Else: Update `shown_count`, `last_shown_at`
5. Return next prompt

**Response:**
```json
{
  "success": true,
  "retired": false,  // true if prompt retired after 3 skips
  "nextPrompt": { ... }
}
```

**Key Design:** After 3 dismissals, prompt permanently retired (won't regenerate due to anchor_hash uniqueness)

---

#### `POST /api/prompts/restore`
**File:** `/app/api/prompts/restore/route.ts` (estimated - file exists)

**Purpose:** User wants to see a dismissed prompt again

**Implementation:** Move from `prompt_history` back to `active_prompts`, reset `shown_count`

---

#### `POST /api/stories` (Prompt Generation)
**File:** `/app/api/stories/route.ts` (lines 221-561)

**Purpose:** Save new story AND trigger prompt generation

**Authentication:** Required

**Request Body:**
```json
{
  "title": "My First Dog",
  "transcript": "...",
  "year": 2020,
  "lessonLearned": "User-selected wisdom text",
  "sourcePromptId": "uuid or null"
}
```

**Logic:**
1. Save story to database
2. If `sourcePromptId`: Mark prompt as used, archive to history
3. **Tier 1:** Extract entities ‚Üí Generate 1-3 prompts ‚Üí Store
4. **Echo:** GPT-4o-mini follow-up ‚Üí Store
5. **Milestone Check:** If story count in [1,2,3,4,7,10...]:
   - Fetch all user stories
   - Call `performTier3Analysis()`
   - Store Tier 3 prompts + character insights

**Response:**
```json
{
  "story": { ... }  // Saved story object
}
```

**Side Effects:**
- 1-4 new prompts in `active_prompts`
- If milestone: Character insights in `character_evolution`

---

#### `POST /api/transcribe`
**File:** `/app/api/transcribe/route.ts` (lines 1-415)

**Purpose:** Transcribe audio file AND extract lessons

**Authentication:** Optional (for FormData uploads from review page)

**Request:**
- **FormData:** `audio` file (up to 25MB)
- OR **JSON:** `{ audioBase64, mimeType }`

**Logic:**
1. Convert audio to file
2. OpenAI Whisper API ‚Üí Raw transcript
3. Parallel:
   - GPT-4o formatting ‚Üí Clean paragraphs
   - GPT-4o lesson extraction ‚Üí 3 options
4. Delete temp file

**Response:**
```json
{
  "transcription": "Cleaned transcript with proper paragraphs...",
  "duration": 180,  // Estimated seconds
  "lessonOptions": {
    "practical": "Check the oil before long trips",
    "emotional": "Fear of failure drives preparation",
    "character": "Responsibility means anticipating problems"
  }
}
```

**Cost:** ~$0.01 per transcription (Whisper $0.006/min + GPT-4o $0.005)

---

### Pending Implementation

#### `GET /api/prompts/locked`
**Purpose:** Fetch locked prompts for paywall display

**Logic:** Query `active_prompts WHERE is_locked = true AND user_id = X`

**Use Case:** Story 3 shows "You have 3 more insights waiting. Subscribe to unlock."

---

#### `POST /api/prompts/on-demand` (Tier 2)
**Purpose:** User requests new prompt manually

**Rate Limit:** Once per 24 hours (check `users.last_tier2_attempt`)

**Logic:**
1. Fetch last 3 stories
2. Fetch existing active prompts
3. GPT-4o: "What haven't we asked yet?"
4. Generate 1-2 new prompts
5. Update `last_tier2_attempt`

---

#### `POST /api/prompts/block-topic`
**Purpose:** User adds topic to "do not ask" list

**Request Body:**
```json
{
  "topic": "my divorce"
}
```

**Logic:**
1. Append to `users.do_not_ask` JSONB array
2. Delete any active prompts matching this topic (requires NLP or keyword matching)

---

#### `GET /api/character-evolution`
**Purpose:** Fetch character insights timeline for display in app

**Response:**
```json
{
  "milestones": [
    {
      "story_count": 1,
      "traits": [...],
      "invisible_rules": [...]
    },
    {
      "story_count": 10,
      "traits": [...],  // Compare evolution
      "invisible_rules": [...]
    }
  ]
}
```

---

## OpenAI Integration Details

### Models Used

**1. Whisper-1 (Audio Transcription)**
- **Purpose:** Convert audio to text
- **File:** `/app/api/transcribe/route.ts` (line 354)
- **Pricing:** $0.006 per minute
- **Max File Size:** 25MB
- **Input:** Audio file (webm, mp3, wav, m4a)
- **Output:** Raw transcript text
- **Example:**
  ```typescript
  const transcription = await openai.audio.transcriptions.create({
    file: audioReadStream,
    model: "whisper-1"
  });
  ```

**2. GPT-4o (Heavy Analysis)**
- **Purpose:** Transcript formatting, lesson extraction, Tier 3 analysis
- **Files:**
  - `/app/api/transcribe/route.ts` (lines 99, 144): Formatting + lesson extraction
  - `/lib/tier3Analysis.ts` (line 103): Milestone analysis
- **Pricing:** $5.00 per 1M input tokens, $15.00 per 1M output tokens
- **Temperature:** 0.3 (formatting), 0.7 (Tier 3 analysis), 0.8 (lesson extraction)
- **Response Format:** JSON object (for Tier 3), plain text (for formatting)
- **Example:**
  ```typescript
  const completion = await openai.chat.completions.create({
    model: "gpt-4o",
    temperature: 0.7,
    response_format: { type: "json_object" },
    messages: [
      { role: "system", content: systemPrompt },
      { role: "user", content: userPrompt }
    ]
  });
  ```

**3. GPT-4o-mini (Fast Echo Prompts)**
- **Purpose:** Instant follow-up questions after every story
- **File:** `/lib/echoPrompts.ts` (line 19)
- **Pricing:** $0.150 per 1M input tokens, $0.600 per 1M output tokens
- **Temperature:** 0.4 (balanced creativity)
- **Max Tokens:** 50 (keep prompts short)
- **Why mini:** 10x cheaper than GPT-4o, fast response for immediate engagement
- **Example:**
  ```typescript
  const response = await openai.chat.completions.create({
    model: "gpt-4o-mini",
    messages: [
      { role: "system", content: "You are a caring grandchild..." },
      { role: "user", content: `Generate one follow-up question for: "${lastSection}"` }
    ],
    max_tokens: 50,
    temperature: 0.4
  });
  ```

---

### Cost per Operation

**Transcription + Formatting + Lesson Extraction (per story):**
- Whisper (5 min audio): $0.03
- GPT-4o formatting (500 tokens in, 300 out): $0.007
- GPT-4o lesson extraction (500 tokens in, 100 out): $0.004
- **Total per story:** ~$0.04

**Echo Prompt (per story):**
- GPT-4o-mini (400 tokens in, 50 out): $0.00009
- **Total:** ~$0.0001 (negligible)

**Tier 1 Prompts (per story):**
- Regex only, no API calls
- **Total:** $0.00

**Tier 3 Analysis (per milestone):**
- GPT-4o input (3 stories √ó 500 tokens = 1500 tokens): $0.0075
- GPT-4o output (500 tokens): $0.0075
- **Total per milestone:** ~$0.015
- **Frequency:** [1, 2, 3, 4, 7, 10, 15, 20, 30, 50, 100] = 11 times per user lifetime

**Total AI Cost per User (10 stories):**
- 10 stories √ó $0.04 = $0.40 (transcription + formatting + lesson)
- 10 echoes √ó $0.0001 = $0.001
- 3 Tier 3 analyses (milestones 1, 2, 3, 4, 7, 10) = 6 √ó $0.015 = $0.09
- **Total:** ~$0.49 per user (10 stories)

**At Scale (10,000 users, avg 10 stories each):**
- 10,000 users √ó $0.49 = **$4,900 total AI cost**
- Revenue (10% convert to $149/year): 1,000 √ó $149 = **$149,000**
- **AI cost as % of revenue:** 3.3% (very healthy margin)

---

### GPT-5 Upgrade: Reasoning for Deeper Synthesis (October 2025)

**Status:** Production-ready with feature flags  
**Branch:** `feature/gpt5-tier3-whispers`  
**Commit:** eeb7493  

#### Overview

HeritageWhisper now uses GPT-5 with adjustable reasoning effort for operations requiring deeper synthesis (Tier-3 milestone analysis and Whisper generation), while maintaining fast performance on high-frequency operations (Tier-1 and Echo prompts).

#### Model Routing Strategy

**Fast Operations (Always gpt-4o-mini):**
- ‚úÖ **Tier-1 Templates** - Entity-based prompts after every story save
- ‚úÖ **Echo Prompts** - Instant sensory follow-up questions
- **Reasoning:** No reasoning effort (optimized for speed and cost)
- **Why:** These run on every story save; speed matters more than depth

**Deep Synthesis Operations (GPT-5 when enabled):**
- ‚úÖ **Tier-3 Milestone Analysis** - Stories 1, 2, 3, 4, 7, 10, 15, 20, 30, 50, 100
  - **Reasoning Effort Progression:**
    - Stories 1-9: `low` effort (basic pattern recognition)
    - Stories 10-49: `medium` effort (pattern synthesis)
    - Stories 50+: `high` effort (deep character insights)
- ‚úÖ **Whisper Generation** - Context-aware prompts based on story content
  - **Reasoning Effort:** Fixed at `medium`
- **Why:** These run infrequently; quality matters more than speed

#### Architecture

**New Files:**
```
lib/ai/
‚îú‚îÄ‚îÄ modelConfig.ts        # Model selection & reasoning effort mapping
‚îî‚îÄ‚îÄ gatewayClient.ts      # Gateway client with telemetry extraction

tests/ai/
‚îú‚îÄ‚îÄ routing.spec.ts       # Model routing tests
‚îî‚îÄ‚îÄ gatewayClient.spec.ts # Gateway connection tests
```

**Updated Files:**
- `/lib/tier3AnalysisV2.ts` - Now uses GPT-5 with adjustable reasoning effort
- `/lib/whisperGeneration.ts` - Now uses GPT-5 at medium effort
- `/lib/echoPrompts.ts` - Updated to use Gateway (stays on fast model)
- `/app/api/stories/route.ts` - Added comprehensive telemetry logging

#### Configuration

**Environment Variables:**
```bash
# Vercel AI Gateway (required)
VERCEL_AI_GATEWAY_BASE_URL=https://ai-gateway.vercel.sh/v1
VERCEL_AI_GATEWAY_API_KEY=your_vercel_ai_gateway_key

# Model Configuration
NEXT_PUBLIC_FAST_MODEL_ID=gpt-4o-mini          # Default for Tier-1/Echo
NEXT_PUBLIC_GPT5_MODEL_ID=gpt-5                # Default for Tier-3/Whispers

# Feature Flags (safe defaults: false = use fast model)
NEXT_PUBLIC_GPT5_TIER3_ENABLED=true            # Enable GPT-5 for Tier-3
NEXT_PUBLIC_GPT5_WHISPERS_ENABLED=true         # Enable GPT-5 for Whispers
```

**Reasoning Effort Logic:**
```typescript
export function effortForMilestone(milestone: number): ReasoningEffort {
  if (milestone >= 50) return "high";   // Stories 50+: Deep synthesis
  if (milestone >= 10) return "medium"; // Stories 10-49: Pattern recognition
  return "low";                         // Stories 1-9: Basic analysis
}
```

#### Telemetry & Observability

**Per-Call Telemetry (logged to console + Gateway dashboard):**
```typescript
{
  op: "ai_call",
  stage: "tier3" | "whisper" | "echo",
  milestone?: number,
  model: "gpt-5" | "gpt-4o-mini",
  effort: "low" | "medium" | "high" | "n/a",
  ttftMs: 150,              // Time to first token
  latencyMs: 2500,          // Total request latency
  costUsd: 0.0234,          // Cost in USD
  tokensUsed: {
    input: 1500,
    output: 450,
    reasoning: 2800,        // GPT-5 reasoning tokens (not visible)
    total: 4750
  }
}
```

**Where to Monitor:**
- **Gateway Dashboard:** https://vercel.com/dashboard/ai-gateway
- **Application Logs:** Server console shows per-call telemetry
- **Cost Tracking:** Real-time spending by model/operation

#### Cost Analysis with GPT-5

**Per Story Costs (GPT-5 Enabled):**
- Transcription (Whisper): $0.03
- Formatting (gpt-4o-mini): $0.001
- Lesson extraction (gpt-4o-mini): $0.001
- Echo prompt (gpt-4o-mini): $0.0001
- **Tier-1** (regex): $0.00
- **Total per story:** ~$0.032

**Tier-3 Milestone Costs (GPT-5 with reasoning effort):**
- **Story 3** (low effort): ~$0.02 per analysis
- **Story 10** (medium effort): ~$0.05 per analysis
- **Story 50** (high effort): ~$0.15 per analysis

**Whisper Generation Costs (GPT-5 at medium effort):**
- Per whisper: ~$0.01

**Total Cost per User (10 stories, GPT-5 enabled):**
- 10 stories √ó $0.032 = $0.32 (transcription + formatting + echo)
- 10 whispers √ó $0.01 = $0.10 (GPT-5 whispers)
- 6 Tier-3 analyses (milestones 1,2,3,4,7,10):
  - Story 1,2,3 (low): 3 √ó $0.02 = $0.06
  - Story 4,7 (low): 2 √ó $0.02 = $0.04
  - Story 10 (medium): 1 √ó $0.05 = $0.05
- **Total:** ~$0.57 per user (10 stories)

**Cost Comparison:**
- **Baseline** (all gpt-4o-mini): $0.49 per user
- **With GPT-5**: $0.57 per user (+16% cost increase)
- **Value:** Significantly better prompt quality ‚Üí higher Story 3 conversion

**At Scale (10,000 users, GPT-5 enabled):**
- 10,000 users √ó $0.57 = **$5,700 total AI cost**
- Revenue (10% convert): 1,000 √ó $149 = **$149,000**
- **AI cost as % of revenue:** 3.8% (still very healthy)

**Cost Controls:**
- Disable Whispers: `NEXT_PUBLIC_GPT5_WHISPERS_ENABLED=false` saves $0.10/user
- Reduce Tier-3 frequency: Adjust milestone thresholds
- Lower effort mapping: Modify `effortForMilestone()` to use medium at 50+ only

#### Performance Characteristics

**Latency Expectations:**
- **Tier-1** (regex): <100ms (unchanged)
- **Echo** (gpt-4o-mini): ~500ms (unchanged)
- **Tier-3** (GPT-5):
  - Low effort (Stories 1-9): 2-5 seconds
  - Medium effort (Stories 10-49): 5-8 seconds
  - High effort (Stories 50+): 8-15 seconds
- **Whispers** (GPT-5 medium): ~2-3 seconds

**Quality Improvements:**
- **Tier-3 Prompts:** Better pattern recognition across multiple stories
- **Character Insights:** Deeper analysis of invisible rules and contradictions
- **Whispers:** More sophisticated detection of implicit emotional content
- **Conversion Impact:** Expected 5-10% increase in Story 3 ‚Üí paid conversion

#### Feature Flags & Rollback

**Safe Defaults:**
```typescript
export const flags = {
  GPT5_TIER3_ENABLED: process.env.NEXT_PUBLIC_GPT5_TIER3_ENABLED === "true",
  GPT5_WHISPERS_ENABLED: process.env.NEXT_PUBLIC_GPT5_WHISPERS_ENABLED === "true",
};
```

If flags are `false` (or not set), system uses gpt-4o-mini for all operations.

**Immediate Rollback:**
```bash
# Set in Vercel dashboard ‚Üí redeploy
NEXT_PUBLIC_GPT5_TIER3_ENABLED=false
NEXT_PUBLIC_GPT5_WHISPERS_ENABLED=false
```

No code changes needed for rollback.

#### Monitoring Alerts

**Watch for:**
- **Tier-3 latency spikes:** >15s indicates potential issues
- **Prompt quality scores:** Should maintain ‚â•70 average
- **Story 3 conversion:** Target ‚â•45% (baseline: 35-40%)
- **Error rates:** GPT-5 should have <1% failure rate
- **Cost per user:** Track in Gateway dashboard by model

#### Testing

**Run Tests:**
```bash
npm test tests/ai/routing.spec.ts        # Model routing & effort mapping
npm test tests/ai/gatewayClient.spec.ts  # Gateway client config
```

**Test Coverage:**
- ‚úÖ Effort mapping: 3‚Üílow, 10‚Üímedium, 50‚Üíhigh
- ‚úÖ Model selection respects feature flags
- ‚úÖ Fast model always used for Tier-1/Echo
- ‚úÖ Gateway client configuration
- ‚úÖ No linting errors

#### Documentation

**Full Implementation Guide:** `/GPT5_FEATURE_README.md`

**Includes:**
- Complete environment setup
- Cost implications by scale
- Troubleshooting guide
- Gateway dashboard access
- Rollback procedures

---

### Rate Limits & Error Handling

**Rate Limiting:**
- **Location:** `/lib/ratelimit.ts`
- **Implementation:** Upstash Redis (graceful fallback if not configured)
- **Limits:**
  - Auth endpoints: 5 requests per 10 seconds
  - Upload endpoints: 10 requests per minute
  - API endpoints: 30 requests per minute
  - Transcribe: 30 per minute per user

**OpenAI API Error Handling:**

**1. Retry Logic (None - by design):**
- No automatic retries to avoid cost explosion
- User can retry transcription manually if it fails

**2. Fallback Mechanisms:**
```typescript
// Formatting fallback (line 287 in transcribe/route.ts)
const [formattedText, lessonOptions] = await Promise.all([
  formatTranscription(transcription.text).catch(error => {
    logger.warn("Failed to format transcription, using raw text:", error);
    return transcription.text; // Use raw text if GPT-4o fails
  }),
  generateLessonOptions(transcription.text).catch(error => {
    logger.warn("Failed to generate lesson options:", error);
    return null; // Show generic lessons if GPT-4o fails
  })
]);

// Generic lesson fallbacks (line 151 in transcribe/route.ts)
if (!practical || !emotional || !character) {
  practical = practical || "Every experience teaches something if you're willing to learn from it";
  emotional = emotional || "The heart remembers what the mind forgets";
  character = character || "Who you become matters more than what you achieve";
}
```

**3. Tier 3 Analysis Failures:**
```typescript
// Line 509 in stories/route.ts
try {
  const tier3Result = await performTier3Analysis(allStories, storyCount);
  await storeTier3Results(supabaseAdmin, user.id, storyCount, tier3Result);
} catch (tier3Error) {
  logger.error("[Stories API] Tier 3 analysis failed:", tier3Error);
  // Don't fail the request - Tier 3 is bonus functionality
}
```

**4. Prompt Injection Protection:**
```typescript
// Line 76 in transcribe/route.ts
const sanitizedText = sanitizeUserInput(rawText);

if (!validateSanitizedInput(sanitizedText)) {
  logger.warn("Potential prompt injection detected");
  return rawText; // Return unsanitized original, skip AI processing
}
```

---

## Prompt Quality Scoring

**Purpose:** Predict likelihood user will record from this prompt (0-100)

**Location:** Tier 3 analysis only (GPT-4o generates score)

**Scoring Algorithm (GPT-4o's internal logic):**
- Specificity: References actual names/details from stories (+20)
- Emotional resonance: Touches on unresolved feelings (+15)
- Gap filling: Asks about implied but unshared moments (+15)
- Pattern recognition: Connects 2+ stories (+20)
- Conversation natural: Feels like a caring grandchild asking (+15)
- No generic therapy speak: Not "How did that make you feel?" (+15)

**Threshold Values:**
- **85-100:** Strong pattern, specific details, high emotional weight
- **70-84:** Solid entity expansion, references actual names
- **60-69:** Decade gap filler, less specific
- **0-59:** Generic, unlikely to trigger recording (shouldn't be generated)

**Example Prompt Scores:**
- 95: "You learned responsibility from Chewy, then tested it with a newborn. How did those early lessons shape your parenting?" (Tier 3, connects 2 stories)
- 85: "What did Chewy look like?" (Tier 1, specific entity)
- 75: "You mentioned feeling 'housebroken by love'‚Äîwhen did you first feel that?" (Echo, uses exact quote)
- 60: "What was a typical Saturday like in the 1980s?" (Tier 0, decade fallback)

**Display Priority:**
```sql
-- Query in /api/prompts/next (line 82)
SELECT * FROM active_prompts
WHERE user_id = ? AND is_locked = false AND expires_at > NOW()
ORDER BY tier DESC, prompt_score DESC
LIMIT 1;
```

**Why It Matters:**
- Higher scores shown first
- Analytics: Track conversion rate by score bucket
- Future: Auto-retire prompts with score < 60 if not used within 3 days

---

## Deduplication System

**Purpose:** Prevent duplicate prompts (e.g., "What did Chewy look like?" appearing twice)

**Method:** SHA1 hash of `type|entity|year`

**Location:** `/lib/promptGeneration.ts` (lines 435-445)

```typescript
export function generateAnchorHash(
  type: string,       // "person_expansion", "place_memory", etc.
  entity: string,     // "Chewy", "the workshop", "Coach"
  year: number | null // 2020, 1975, or null
): string {
  const normalized = entity.toLowerCase().trim();  // Case-insensitive
  const yearStr = year ? year.toString() : "NA";   // Year or "NA"
  const input = `${type}|${normalized}|${yearStr}`;
  
  return createHash("sha1").update(input).digest("hex");
}
```

**Examples:**
```typescript
generateAnchorHash("person_expansion", "Chewy", 2020)
// ‚Üí "060b0a0d4a075bad6933f9dbc0edc96b3622c299"

generateAnchorHash("person_expansion", "chewy", 2020)  // lowercase
// ‚Üí "060b0a0d4a075bad6933f9dbc0edc96b3622c299"  // SAME HASH

generateAnchorHash("person_expansion", "Chewy", 1990)
// ‚Üí "a1b2c3d4e5f6..."  // DIFFERENT HASH (different year)

generateAnchorHash("place_memory", "the workshop", null)
// ‚Üí "f1e2d3c4b5a6..."
```

**Database Constraint:**
```sql
-- Line 87 in migration
CONSTRAINT active_prompts_unique_anchor UNIQUE(user_id, anchor_hash)
```

**Collision Prevention:**
- Same entity + same year + same type = same hash = duplicate caught
- Different year = different hash = allowed (e.g., "What did Chewy look like?" for 2020 vs 1990 story)
- Different type = different hash = allowed (e.g., "person_expansion" vs "place_memory" for "Chewy")

**Insertion Logic:**
```typescript
// Line 367 in stories/route.ts
const { error: promptError } = await supabaseAdmin
  .from("active_prompts")
  .insert(promptsToInsert);

if (promptError) {
  if (promptError.code === "23505") {  // Duplicate key violation
    logger.debug("[Stories API] Some prompts already exist (deduplication working)");
  } else {
    logger.error("[Stories API] Failed to store prompts:", promptError);
  }
}
```

**Edge Cases:**
1. **Typo in entity name:** "Chewey" vs "Chewy" ‚Üí Different hashes ‚Üí Allowed (could generate 2 prompts)
2. **Echo prompts:** Use last 100 chars of transcript ‚Üí Unique per story
3. **Tier 3 prompts:** Use actual entity name from AI extraction ‚Üí Consistent across stories

**Future Enhancement:** Fuzzy matching for entity names (Levenshtein distance < 2 = same entity)

---

## Testing & Debugging

### Key Functions to Test

**1. Entity Extraction:**
```typescript
// Input: Story transcript
const transcript = "I remember getting Chewy, our first dog...";

// Expected output:
const entities = extractEntities(transcript);
// {
//   people: ["Chewy", "our first dog"],
//   places: [],
//   objects: [],
//   emotions: [],
//   temporalBoundaries: ["first"]
// }

// Test edge cases:
// - "Coach Thompson" ‚Üí Extracts as "Coach Thompson"
// - "my father" ‚Üí Extracts as "my father" (not just "father")
// - "the girl" ‚Üí Extracts correctly
```

**2. Anchor Hash Generation:**
```typescript
const hash1 = generateAnchorHash("person_expansion", "Chewy", 2020);
const hash2 = generateAnchorHash("person_expansion", "chewy", 2020);
expect(hash1).toBe(hash2);  // Case-insensitive

const hash3 = generateAnchorHash("person_expansion", "Chewy", 1990);
expect(hash1).not.toBe(hash3);  // Different year = different hash
```

**3. Tier 3 Milestone Detection:**
```typescript
const MILESTONES = [1, 2, 3, 4, 7, 10, 15, 20, 30, 50, 100];
expect(MILESTONES.includes(3)).toBe(true);  // Trigger Tier 3
expect(MILESTONES.includes(5)).toBe(false); // Skip
```

**4. Prompt Skip Retirement:**
```typescript
// Simulate 3 skips
await skipPrompt(promptId);  // shown_count = 1
await skipPrompt(promptId);  // shown_count = 2
await skipPrompt(promptId);  // shown_count = 3 ‚Üí retired to history

const { data: active } = await supabase
  .from("active_prompts")
  .select("*")
  .eq("id", promptId);
expect(active.length).toBe(0);  // Deleted from active

const { data: history } = await supabase
  .from("prompt_history")
  .select("*")
  .eq("anchor_hash", promptHash);
expect(history.length).toBe(1);  // Archived
expect(history[0].outcome).toBe("skipped");
```

---

### Common Failure Points

**1. Duplicate Prompt Errors**
- **Issue:** Database error code 23505 when inserting prompts
- **Cause:** Same entity mentioned in multiple stories
- **Expected Behavior:** Silently skip duplicate (deduplication working)
- **Fix:** Check for error code 23505, log as debug (not error)
- **Location:** `/app/api/stories/route.ts` (lines 367-380)

**2. OpenAI API Timeouts**
- **Issue:** Tier 3 analysis takes >30 seconds, Vercel function times out
- **Cause:** Multiple long stories (10+ stories at milestone 10)
- **Fix:** Implement background job for Tier 3 (Vercel Cron or Queue)
- **Workaround:** Truncate stories to last 500 words each for analysis

**3. Echo Prompt Generation Fails**
- **Issue:** `generateEchoPrompt()` returns null
- **Cause:** GPT-4o-mini response too long (>150 chars) or empty
- **Fix:** Gracefully skip echo prompt, Tier 1 still generated
- **Location:** `/lib/echoPrompts.ts` (lines 56-58)

**4. Entity Extraction Misses Names**
- **Issue:** Story mentions "my wife Sarah" but only extracts "my wife"
- **Cause:** Regex pattern doesn't capture proper name after role noun
- **Fix:** Add pattern: `/\b(my|his|her)\s+(wife|husband)\s+([A-Z][a-z]+)/`
- **Location:** `/lib/promptGeneration.ts` (line 93)

**5. Prompt History Not Tracking Source**
- **Issue:** `source_prompt_id` in stories table is NULL
- **Cause:** User didn't record from prompt, or client didn't pass promptId
- **Expected:** Most stories won't have source_prompt_id (only ~30% conversion)
- **Fix:** Not a bug‚Äîorganic stories don't have prompt attribution

---

### Debug Queries

**Check active prompts for user:**
```sql
SELECT 
  id, 
  prompt_text, 
  anchor_entity,
  tier,
  prompt_score,
  shown_count,
  expires_at
FROM active_prompts 
WHERE user_id = '38ad3036-e423-4e41-a3f3-020664a1ee0e'
ORDER BY tier DESC, prompt_score DESC;
```

**Check prompt history (outcomes):**
```sql
SELECT 
  prompt_text, 
  outcome, 
  tier,
  shown_count,
  resolved_at
FROM prompt_history 
WHERE user_id = '38ad3036-e423-4e41-a3f3-020664a1ee0e'
ORDER BY resolved_at DESC
LIMIT 20;
```

**Find duplicate prompts (shouldn't happen):**
```sql
SELECT 
  anchor_hash, 
  COUNT(*) as count,
  ARRAY_AGG(prompt_text) as prompts
FROM active_prompts
WHERE user_id = '38ad3036-e423-4e41-a3f3-020664a1ee0e'
GROUP BY anchor_hash
HAVING COUNT(*) > 1;
```

**Check character evolution timeline:**
```sql
SELECT 
  story_count,
  jsonb_array_length(traits) as trait_count,
  jsonb_array_length(invisible_rules) as rule_count,
  analyzed_at
FROM character_evolution
WHERE user_id = '38ad3036-e423-4e41-a3f3-020664a1ee0e'
ORDER BY story_count ASC;
```

**Prompt conversion rate by tier:**
```sql
SELECT 
  tier,
  COUNT(*) as total,
  SUM(CASE WHEN outcome = 'used' THEN 1 ELSE 0 END) as used,
  ROUND(
    SUM(CASE WHEN outcome = 'used' THEN 1 ELSE 0 END)::float / COUNT(*) * 100, 
    2
  ) as conversion_rate
FROM prompt_history
GROUP BY tier
ORDER BY tier DESC;
```

**Time to conversion (average days):**
```sql
SELECT 
  AVG(EXTRACT(EPOCH FROM (resolved_at - created_at)) / 86400) as avg_days_to_use
FROM prompt_history
WHERE outcome = 'used';
```

---

## Quick Troubleshooting Guide

### "I'm not seeing any prompts"

**Check 1: Are there active prompts in the database?**
```sql
SELECT COUNT(*) FROM active_prompts 
WHERE user_id = 'YOUR_USER_ID' 
AND expires_at > NOW() 
AND is_locked = false;
```
- If 0: User has no active prompts, check if they've saved any stories
- If >0: Frontend issue‚Äîcheck API call to `/api/prompts/next`

**Check 2: Are prompts expired?**
```sql
SELECT COUNT(*) FROM active_prompts 
WHERE user_id = 'YOUR_USER_ID' 
AND expires_at < NOW();
```
- If >0: Run cleanup function to archive expired prompts:
  ```sql
  SELECT archive_expired_prompts();
  ```

**Check 3: Are all prompts locked?**
```sql
SELECT COUNT(*) FROM active_prompts 
WHERE user_id = 'YOUR_USER_ID' 
AND is_locked = true;
```
- If >0: Story 3 paywall‚Äîuser needs to subscribe to unlock

---

### "Tier 3 analysis didn't run at milestone"

**Check 1: Did milestone detection trigger?**
```bash
# Check logs for this line (stories/route.ts line 476)
grep "MILESTONE HIT" logs.txt
```
- If not found: Milestone logic didn't trigger‚Äîcheck story count

**Check 2: Story count correct?**
```sql
SELECT COUNT(*) FROM stories WHERE user_id = 'YOUR_USER_ID';
```
- If count is NOT in [1,2,3,4,7,10,15,20,30,50,100]: No milestone triggered

**Check 3: OpenAI API failure?**
```bash
# Check logs for Tier 3 errors
grep "Tier 3 analysis failed" logs.txt
```
- Common causes: OpenAI timeout, API key invalid, rate limit exceeded

**Check 4: Character insights stored?**
```sql
SELECT story_count, analyzed_at 
FROM character_evolution 
WHERE user_id = 'YOUR_USER_ID';
```
- If missing: Tier 3 ran but failed to store results

---

### "Same prompt appearing multiple times"

**Check 1: Deduplication working?**
```sql
-- Should return 0 rows
SELECT anchor_hash, COUNT(*) 
FROM active_prompts 
WHERE user_id = 'YOUR_USER_ID'
GROUP BY anchor_hash 
HAVING COUNT(*) > 1;
```
- If >0: Database constraint failed (shouldn't happen)

**Check 2: User skipped but didn't retire?**
```sql
SELECT prompt_text, shown_count, last_shown_at
FROM active_prompts 
WHERE user_id = 'YOUR_USER_ID'
AND shown_count >= 3;
```
- If >0: Skip logic didn't archive prompt after 3 dismissals

**Check 3: localStorage dismissals not syncing?**
- User dismisses in UI but API call fails
- Check: Clear localStorage, refresh page

---

### "Echo prompts too generic"

**Check 1: Is GPT-4o-mini returning vague questions?**
```typescript
// Add debug logging to echoPrompts.ts line 61
console.log("[Echo Debug] Generated:", promptText);
console.log("[Echo Debug] Input (last 300 words):", lastSection);
```

**Check 2: Transcript quality poor?**
- If Whisper transcription is garbled, echo prompt will be generic
- Solution: Improve audio quality, check microphone settings

**Check 3: Temperature too low?**
- Current: `temperature: 0.4`
- Try: Increase to 0.6 for more creative questions
- Location: `/lib/echoPrompts.ts` line 51

---

### "Lesson extraction returning generic fallbacks"

**Check 1: Is GPT-4o parsing response correctly?**
```typescript
// Add debug logging to transcribe/route.ts line 135
console.log("[Lesson Debug] Raw response:", content);
console.log("[Lesson Debug] Parsed:", { practical, emotional, character });
```

**Check 2: Response format incorrect?**
- GPT-4o might not follow "PRACTICAL: ... EMOTIONAL: ..." format
- Solution: Adjust system prompt to be more explicit

**Check 3: Story too short?**
- Transcripts <100 words may not have extractable lessons
- Solution: Add check: If transcript.split(' ').length < 100, skip lesson extraction

---

## Performance Metrics

### Current Performance

**Average Prompt Generation Time:**
- **Tier 1 (template):** <100ms (regex only, no API)
- **Echo (GPT-4o-mini):** ~500ms
- **Tier 3 (GPT-4o):** 3-8 seconds (depends on story count)

**API Costs per User (10 stories):**
- Transcription: $0.30 (10 stories √ó $0.03)
- Formatting: $0.07 (10 stories √ó $0.007)
- Lesson extraction: $0.04 (10 stories √ó $0.004)
- Echo prompts: $0.001 (10 stories √ó $0.0001)
- Tier 3 analysis: $0.09 (6 milestones √ó $0.015)
- **Total:** $0.50 per user (10 stories)

**Database Query Performance:**
- `/api/prompts/next`: ~50ms (indexed query on tier + prompt_score)
- Entity extraction: <10ms (in-memory regex)
- Prompt insertion: ~20ms (bulk insert with deduplication)

**Prompt Conversion Metrics (Estimated):**
- **Tier 3 prompts:** 40% conversion (user records from prompt)
- **Tier 1 prompts:** 25% conversion
- **Echo prompts:** 30% conversion
- **Tier 0 fallback:** 10% conversion

---

### Optimization Opportunities

**1. Batch Tier 3 Processing**
- **Current:** Synchronous GPT-4o call during story save (blocks response)
- **Optimization:** Queue Tier 3 analysis as background job
- **Implementation:** Vercel Cron or Inngest queue
- **Benefit:** Faster story save response (300ms vs 5 seconds)

**2. Cache Entity Extraction Results**
- **Current:** Re-extract entities every time story is accessed
- **Optimization:** Store extracted entities in `stories.entities_extracted` JSONB
- **Benefit:** Reuse for relationship mapping, faster prompt regeneration

**3. Precompute Decade Fallbacks**
- **Current:** Generate decade prompt on-demand when user has no active prompts
- **Optimization:** Pre-generate 1 decade fallback per user at signup
- **Benefit:** Instant prompt display (no API call needed)

**4. Tier 1 Template Variants**
- **Current:** 15 templates per entity type, random selection
- **Optimization:** A/B test templates, remove low-performing ones
- **Benefit:** Higher conversion rate on Tier 1 prompts

**5. Parallel Tier 3 + Story Save**
- **Current:** Save story ‚Üí Check milestone ‚Üí Run Tier 3
- **Optimization:** Save story in parallel with Tier 3 trigger check
- **Benefit:** Shave 200ms off response time

---

## Configuration & Environment Variables

### Required API Keys

```bash
# OpenAI (Required for all AI features)
OPENAI_API_KEY=sk-proj-...

# Supabase (Required for database)
NEXT_PUBLIC_SUPABASE_URL=https://tjycibrhoammxohemyhq.supabase.co
NEXT_PUBLIC_SUPABASE_ANON_KEY=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9...
SUPABASE_SERVICE_ROLE_KEY=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9...

# Upstash Redis (Optional - for rate limiting)
UPSTASH_REDIS_REST_URL=https://...upstash.io
UPSTASH_REDIS_REST_TOKEN=...
```

### Model Selection Options

**Whisper Model:**
- Currently: `whisper-1` (only option available)
- Future: If OpenAI releases `whisper-2`, update in `/app/api/transcribe/route.ts` line 354

**GPT Model for Formatting:**
- Currently: `gpt-4o` (best quality)
- Alternatives: `gpt-4-turbo-preview` (cheaper, slightly lower quality)
- Location: `/app/api/transcribe/route.ts` line 100

**GPT Model for Tier 3:**
- Currently: `gpt-4o` (required for JSON mode + quality)
- Cannot downgrade: Tier 3 requires structured JSON output
- Location: `/lib/tier3Analysis.ts` line 103

**GPT Model for Echo:**
- Currently: `gpt-4o-mini` (optimal cost/speed balance)
- Alternatives: `gpt-3.5-turbo` (cheaper but lower quality)
- Location: `/lib/echoPrompts.ts` line 19

---

### Feature Flags (Future)

```bash
# Enable/disable specific features
ENABLE_TIER_3_ANALYSIS=true
ENABLE_ECHO_PROMPTS=true
ENABLE_CHARACTER_EVOLUTION=true
ENABLE_DO_NOT_ASK=false  # Not yet implemented

# Tier 3 milestone list (comma-separated)
TIER_3_MILESTONES=1,2,3,4,7,10,15,20,30,50,100

# Prompt expiry (days)
TIER_1_EXPIRY_DAYS=7
TIER_3_EXPIRY_DAYS=30

# Skip retirement threshold
SKIP_RETIREMENT_COUNT=3
```

---

## Future Enhancements Roadmap

### Tier 2: On-Demand Prompt Generation
**Status:** Planned, not implemented

**User Story:** "I want more prompts without waiting for a milestone"

**Implementation:**
1. Add "Get More Prompts" button in app
2. Rate limit: 1 request per 24 hours
3. GPT-4o analyzes: Last 3 stories + existing prompts ‚Üí "What haven't we asked?"
4. Generate 1-2 new prompts
5. Track usage in `users.last_tier2_attempt`

**Cost:** $0.01 per on-demand generation

**Business Logic:** Free users get 1 per week, paid users unlimited

---

### Premium Prompt Unlocking
**Status:** Database ready, Stripe integration pending

**User Story:** "I see 3 locked prompts at Story 3, I want to unlock them"

**Implementation:**
1. Story 3 Tier 3 analysis generates 4 prompts
2. 1st prompt: `is_locked = false` (free preview)
3. Prompts 2-4: `is_locked = true` (paywall seed)
4. Display locked prompts with blur effect: "Subscribe ($149/year) to unlock"
5. After payment ‚Üí Update all `is_locked = false`

**Database:** `active_prompts.is_locked` column already exists

**Stripe Webhook:**
```typescript
// Listen for checkout.session.completed
await supabase
  .from("active_prompts")
  .update({ is_locked: false })
  .eq("user_id", userId)
  .eq("is_locked", true);

// Update subscription status
await supabase
  .from("users")
  .update({ subscription_status: "active" })
  .eq("id", userId);
```

---

### Do-Not-Ask Topic Blocking
**Status:** Database ready, UI not built

**User Story:** "Stop asking about my divorce"

**Implementation:**
1. Add "Block This Topic" button on prompt cards
2. Modal: "What topic should we avoid?" (text input)
3. Store in `users.do_not_ask` JSONB array
4. When generating prompts:
   ```typescript
   const blockedTopics = user.do_not_ask || [];
   const filteredPrompts = prompts.filter(p => {
     return !blockedTopics.some(topic => 
       p.prompt_text.toLowerCase().includes(topic.toLowerCase())
     );
   });
   ```

**Database:** `users.do_not_ask` column already exists

**Edge Cases:**
- Block "my father" ‚Üí Also blocks "my dad", "my father's workshop"
- Solution: NLP similarity matching (future enhancement)

---

### Relationship Mapping
**Status:** Concept phase

**User Story:** "Who are all the people I've mentioned? How are they connected?"

**Implementation:**
1. Extract people from all stories: `["Chewy", "Sarah", "Coach Thompson", "my father"]`
2. Build co-occurrence graph: Who appears together in stories?
3. Generate prompts: "You mention Sarah and Coach in the same decade but never together. Did they know each other?"

**Database:** New table `relationships`:
```sql
CREATE TABLE relationships (
  id UUID PRIMARY KEY,
  user_id UUID REFERENCES users(id),
  person_a TEXT NOT NULL,
  person_b TEXT,  -- NULL if single person
  story_ids UUID[] NOT NULL,  -- Stories where they appear
  relationship_type TEXT,  -- "family", "friend", "mentor", "co-worker"
  extracted_at TIMESTAMP DEFAULT NOW()
);
```

---

### Emotional Trajectory Analysis
**Status:** Data collected, not analyzed

**User Story:** "Show me how my emotions changed over the decades"

**Implementation:**
1. Analyze `stories.emotions` array per decade
2. Visualize: Timeline with emotion bubbles
3. Generate prompts: "Your 1960s stories are all joyful, but 1970s are anxious. What changed?"

**Database:** `stories.emotions` already exists (JSONB array)

**Analysis Query:**
```sql
SELECT 
  FLOOR(story_year / 10) * 10 as decade,
  emotion,
  COUNT(*) as count
FROM stories, jsonb_array_elements_text(emotions) as emotion
WHERE user_id = ?
GROUP BY decade, emotion
ORDER BY decade, count DESC;
```

---

### Life Phase Tone Matching
**Status:** Database ready, logic not implemented

**User Story:** "Prompts should feel age-appropriate (don't ask a 5-year-old about 'responsibility')"

**Implementation:**
1. Use `stories.life_phase` to adjust verb tense and tone
2. **Childhood (0-12):** Present tense, sensory focus
   - "What did it smell like?"
   - "What color was it?"
3. **Teen (13-19):** Identity questions
   - "When did you first feel..."
   - "Who did you want to become?"
4. **Adult (20-64):** Responsibility & relationships
   - "How did you balance..."
   - "What did you learn from..."
5. **Senior (65+):** Wisdom & reflection
   - "Looking back, what would you tell your younger self?"

**Database:** `stories.life_phase` column already exists

---

### Prompt Performance Dashboard
**Status:** Analytics queries ready, UI not built

**User Story (Internal):** "Which prompt types convert best?"

**Implementation:**
1. Query `prompt_history` for conversion metrics
2. Dashboard:
   - Conversion rate by tier (Tier 3 vs Tier 1)
   - Most skipped prompt types
   - Average time to conversion
   - Prompt score distribution
3. Use data to optimize template library

**Key Metrics:**
```sql
-- Conversion rate by memory_type
SELECT 
  memory_type,
  COUNT(*) as total,
  SUM(CASE WHEN outcome = 'used' THEN 1 ELSE 0 END) as used,
  ROUND(used::float / total * 100, 2) as conversion_rate
FROM prompt_history
GROUP BY memory_type
ORDER BY conversion_rate DESC;
```

---

## Monitoring & Analytics

### Currently Tracked Metrics

**Prompt Engagement:**
- **shown_count:** Incremented on skip, tracked in `active_prompts` and archived to `prompt_history`
- **outcome:** "used" | "skipped" | "expired" (stored in `prompt_history`)
- **conversion_rate:** `COUNT(outcome='used') / COUNT(*) * 100` by tier/memory_type

**Story Frequency:**
- **stories.created_at:** Timestamp of each story save
- **Time between stories:** Calculated on-demand: `AVG(created_at[n] - created_at[n-1])`

**Character Evolution:**
- **traits over time:** Compare `character_evolution` rows at different `story_count` milestones
- **invisible_rules consistency:** Track which rules appear at multiple milestones

**Tier 3 Performance:**
- **Analysis runtime:** Log duration of `performTier3Analysis()` calls
- **Prompt quality:** Track conversion rate of Tier 3 vs Tier 1 prompts

---

### Key Analytics Queries

**1. Prompt Conversion Funnel:**
```sql
WITH funnel AS (
  SELECT 
    user_id,
    COUNT(*) FILTER (WHERE tier = 1) as tier1_generated,
    COUNT(*) FILTER (WHERE tier = 3) as tier3_generated,
    COUNT(*) FILTER (WHERE tier = 1 AND outcome = 'used') as tier1_used,
    COUNT(*) FILTER (WHERE tier = 3 AND outcome = 'used') as tier3_used
  FROM prompt_history
  GROUP BY user_id
)
SELECT 
  AVG(tier1_used::float / NULLIF(tier1_generated, 0) * 100) as tier1_conversion,
  AVG(tier3_used::float / NULLIF(tier3_generated, 0) * 100) as tier3_conversion
FROM funnel;
```

**2. Time to Conversion (by tier):**
```sql
SELECT 
  tier,
  AVG(EXTRACT(EPOCH FROM (resolved_at - created_at)) / 86400) as avg_days
FROM prompt_history
WHERE outcome = 'used'
GROUP BY tier;
```

**3. Most Effective Prompt Types:**
```sql
SELECT 
  memory_type,
  COUNT(*) as total,
  SUM(CASE WHEN outcome = 'used' THEN 1 ELSE 0 END) as used,
  ROUND(used::float / total * 100, 2) as conversion_rate,
  AVG(prompt_score) as avg_score
FROM prompt_history
WHERE memory_type IS NOT NULL
GROUP BY memory_type
ORDER BY conversion_rate DESC;
```

**4. Paywall Conversion (Story 3):**
```sql
-- Users who hit Story 3 milestone
WITH story3_users AS (
  SELECT DISTINCT user_id 
  FROM character_evolution 
  WHERE story_count = 3
),
-- Users who converted to paid
paid_users AS (
  SELECT id as user_id 
  FROM users 
  WHERE subscription_status = 'active'
)
SELECT 
  COUNT(DISTINCT s3.user_id) as story3_users,
  COUNT(DISTINCT p.user_id) as paid_users,
  ROUND(COUNT(DISTINCT p.user_id)::float / COUNT(DISTINCT s3.user_id) * 100, 2) as conversion_rate
FROM story3_users s3
LEFT JOIN paid_users p ON s3.user_id = p.user_id;
```

**5. Character Evolution Insights Growth:**
```sql
SELECT 
  story_count,
  AVG(jsonb_array_length(traits)) as avg_traits,
  AVG(jsonb_array_length(invisible_rules)) as avg_rules,
  AVG(jsonb_array_length(contradictions)) as avg_contradictions
FROM character_evolution
GROUP BY story_count
ORDER BY story_count;
```

---

### Logging Best Practices

**What to Log:**
- ‚úÖ Tier 1 prompt generation count: `[Tier 1] Generated 3 prompts`
- ‚úÖ Tier 3 milestone hits: `üéØ MILESTONE HIT: Story #10`
- ‚úÖ Echo prompt success/failure: `[Echo] Generated: "What did..."`
- ‚úÖ Deduplication: `[Tier 1] Some prompts already exist (deduplication working)`
- ‚úÖ OpenAI API errors: `[Tier 3] GPT-4o analysis failed: timeout`

**What NOT to Log (Security):**
- ‚ùå User transcripts (PII): `logger.debug("Transcript:", transcript)` ‚Üí Remove
- ‚ùå API keys: Never log `OPENAI_API_KEY`
- ‚ùå User emails/names in production logs

**Log Levels:**
- `logger.debug()`: Development only, verbose details
- `logger.info()`: Production, milestone hits, feature usage
- `logger.warn()`: Expected errors (duplicate prompts, API fallbacks)
- `logger.error()`: Unexpected errors (database failures, OpenAI crashes)

---

### Future Monitoring Enhancements

**1. Real-Time Prompt Dashboard (Admin Panel):**
- Active prompts count per tier
- Conversion rates (live)
- OpenAI API cost tracking
- Failed Tier 3 analyses (alert if >5% fail rate)

**2. User Engagement Metrics:**
- % of users with active prompts
- Average prompts per user
- Days since last prompt interaction
- "Dead" users (no prompts for 30+ days)

**3. A/B Testing Framework:**
- Test different Tier 1 templates
- Compare GPT-4o vs GPT-4-turbo for Tier 3
- Test prompt expiry: 7 days vs 14 days
- Measure impact on conversion rate

**4. Cost Tracking:**
- OpenAI API costs per user (track in database)
- Alert if daily cost >$100 (runaway API usage)
- Project monthly cost based on user growth

---

_Documentation last updated: [Current Date]_
_System version: v1.4 (Production Ready)_
_Codebase analyzed: HeritageWhisperV2 (October 2025)_
